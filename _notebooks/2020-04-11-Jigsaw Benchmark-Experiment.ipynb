{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Jigsaw Benchmark Experiment\n",
    "> Benchmark model experiment for Jigsaw Multilingual Toxic Comment Classification.\n",
    "- toc: true\n",
    "- badges: true\n",
    "- comments: true\n",
    "- author: Aman Arora"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I wanted to try a tiny little experiment of submitting the multilingual model without any training to check the basic score that we get. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I followed the same script to make a submission and got a score of 0.5630. So, this makes me happy that the model we used in the previous post is definitely learning how to classify toxicity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will move forward to try out training our multilingual model for various different languages in the test set to classify toxicity. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Stay tuned for more!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So what are we going to do for the next steps? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **FOCAL LOSS:** From this kernel [here](https://www.kaggle.com/miklgr500/jigsaw-tpu-bert-with-huggingface-and-keras) I found that focal loss is a way to deal with class imbalance which is certainly the case here since we are working with data where only a very small amount of comments are toxic. In tensorflow, the author has defined the focal loss as:\n",
    "\n",
    "```python \n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "def focal_loss(gamma=2., alpha=.2):\n",
    "    def focal_loss_fixed(y_true, y_pred):\n",
    "        pt_1 = tf.where(tf.equal(y_true, 1), y_pred, tf.ones_like(y_pred))\n",
    "        pt_0 = tf.where(tf.equal(y_true, 0), y_pred, tf.zeros_like(y_pred))\n",
    "        return -K.mean(alpha * K.pow(1. - pt_1, gamma) * K.log(pt_1)) - K.mean((1 - alpha) * K.pow(pt_0, gamma) * K.log(1. - pt_0))\n",
    "    return focal_loss_fixed\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **TOKENIZERS:** From the same kernel I found that using the [Tokenizers](https://github.com/huggingface/tokenizers) by Hugging Face is a better option than the default tokenizer in `transformers` library because the tokenizers are ulta-fast. From the docs: \n",
    "> Extremely fast (both training and tokenization), thanks to the Rust implementation. Takes less than 20 seconds to tokenize a GB of text on a server's CPU.\n",
    "\n",
    "Particularly we will be making use of this `fast_encode` function from the [same kernel](https://www.kaggle.com/miklgr500/jigsaw-tpu-bert-with-huggingface-and-keras) referenced before: \n",
    "```python \n",
    "def fast_encode(texts, tokenizer, chunk_size=256, maxlen=512):\n",
    "    tokenizer.enable_truncation(max_length=maxlen)\n",
    "    tokenizer.enable_padding(max_length=maxlen)\n",
    "    all_ids = []\n",
    "    \n",
    "    for i in tqdm(range(0, len(texts), chunk_size)):\n",
    "        text_chunk = texts[i:i+chunk_size].tolist()\n",
    "        encs = tokenizer.encode_batch(text_chunk)\n",
    "        all_ids.extend([enc.ids for enc in encs])\n",
    "    \n",
    "    return np.array(all_ids)\n",
    "```\n",
    "\n",
    "The above function makes use of Hugging Face's [tokenizers library](https://github.com/huggingface/tokenizers) and truncates the text to `max_length`, adds padding if required and finally calls `encode_batch`. Finally, extends the list and returns the encoded ids as a `np.array`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- From the same kernel, we realize that valid and test files are all english translated. And the author is able to get an AUC score of 0.914. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **COSINE LEARNING RATE SCHEDULER:** Try Cosine Learning rate scheduler. The same kernel uses the following LR scheduler: \n",
    "![image.png](./my_icons/LRsched.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **ROC AUC:** Use ROC score for validation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **ENSEMBLE:** Eventually we will have to move towards ensembling models. From the kernel [here](https://www.kaggle.com/hamditarek/ensemble/) the score jumps to 0.943!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **XLM ROBERTA:** This [kernel](https://www.kaggle.com/xhlulu/jigsaw-tpu-xlm-roberta) shows that XLM-Roberta can be used to get a score of 0.9383."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
